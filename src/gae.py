import torch
import torch.nn.functional as F

class GAE:
    """
    Generalized Advantage Estimation (GAE) with eligibility trace to compute the advantages.
    GAE allows you to control the trade-off between bias and variance in advantage estimation.
    (source: https://arxiv.org/abs/1506.02438).
    """

    def __init__(self, gamma: float, lambda_: float):
        """
        Initializes the GAE object.
        :param gamma: The discount factor for future rewards.
        :param lambda_: The GAE parameter, ontrols the trade-off between bias and variance in advantage estimation.
        It is a hyperparameter that ranges from 0 to 1. When lambda_ is set to 1, it corresponds to Monte Carlo sampling,
        which has high variance and no bias. On the other hand, when lambda_ is set to 0, it corresponds to normal TD-Learning,
        which has low variance but is biased because the estimates are generated by a neural network.
        """
        self.gamma = gamma
        self.lambda_ = lambda_

    def calculate_gae_eligibility_trace(
        self,
        rewards: torch.Tensor,
        value_preds: torch.Tensor,
        masks: torch.Tensor,
        normalize: bool = False,
    ) -> torch.Tensor:
        """
        Calculate the Generalized Advantage Estimation (GAE) with eligibility trace.

        :param rewards: Tensor of shape [batch_size] containing rewards.
        :param value_preds: Tensor of shape [batch_size] containing value function predictions.
        :param masks: Tensor of shape [batch_size] indicating whether the episode is terminated.
        :param normalize: Whether to normalize the advantage values (optional).
        :return: Tensor of shape [batch_size] containing the advantages.
        """
        advantages = torch.zeros_like(rewards)
        e = torch.zeros_like(rewards)
        gae = 0.0

        for t in range(rewards.shape[0] - 1):
            # Compute the TD error
            td_error = rewards[t] + self.gamma * masks[t] * value_preds[t + 1] - value_preds[t]

            # Update the eligibility trace vector
            e[t + 1] = self.gamma * self.lambda_ * masks[t] * e[t] + td_error

            # Update the GAE variable
            gae = gae + e[t + 1]

            # Update the advantages array using the eligibility trace vector
            advantages[t] = gae

        if normalize:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        return advantages

    def calculate_returns(
        self,
        rewards: torch.Tensor,
        mask: torch.Tensor,
    ) -> torch.Tensor:
        """
        Calculate the Returns based on the rewards.

        :param rewards: Tensor of shape [batch_size] containing rewards.
        :param mask: Tensor of shape [batch_size] indicating whether the episode is terminated.
        :return: Tensor of shape [batch_size] containing the returns.
        """
        # The returns tensor is initialized with zeros, and the last element is set to the corresponding reward value.
        # This initialization allows for correct handling of the termination of episodes.
        returns = torch.zeros_like(rewards)
        returns[-1] = rewards[-1]

        for t in reversed(range(rewards.shape[0] - 1)):
            returns[t] = rewards[t] + self.gamma * mask[t] * returns[t + 1]

        return returns
